{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b55daef",
   "metadata": {},
   "source": [
    "# Code that generate the figures from Denis et al. 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b744d9a8",
   "metadata": {},
   "source": [
    "In this notebook not all the figure were generated with the code bellow. Some figure were generated using Rscript and snakemake pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce32d89d",
   "metadata": {},
   "source": [
    "## Figure 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f94051d",
   "metadata": {},
   "source": [
    "### Figure 3A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c9fc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "import pytaxonkit\n",
    "from Bio import SeqIO\n",
    "from collections import defaultdict\n",
    "import polars.selectors as cs\n",
    "import seaborn.objects as so\n",
    "from seaborn import axes_style\n",
    "from seaborn import plotting_context\n",
    "\n",
    "depth_df = []\n",
    "\n",
    "folder_depth = Path(\"05-results/taxonomic_annotation/empirical_data_bwa/depth_breadth/\")\n",
    "\n",
    "sorted_files = sorted(folder_depth.glob(\"*depth_breadth.tsv\"))\n",
    "\n",
    "for file in sorted_files:\n",
    "    depth_df.append(pl.read_csv(file, separator=\"\\t\"))\n",
    "    depth_df[-1] = depth_df[-1].with_columns(\n",
    "        pl.col('Breadth').cast(pl.Float32),\n",
    "        pl.col('Depth avg').cast(pl.Float32),\n",
    "        pl.col('Depth median').cast(pl.Float32),\n",
    "        sample=pl.lit(file.stem.split('.')[0]),\n",
    "        level=pl.lit(file.stem.split('.')[2]),\n",
    "    )\n",
    "\n",
    "depth_df = pl.concat(depth_df)\n",
    "\n",
    "depth_df = depth_df.rename(\n",
    "    {\n",
    "        \"Contig\": \"seqid\",\n",
    "    }\n",
    ")\n",
    "\n",
    "depth_df = depth_df.filter(~pl.col('level').eq('unclassified'))\n",
    "\n",
    "depth_df = depth_df.with_columns(\n",
    "    seqid_sample=pl.col('seqid') + pl.lit('_') + pl.col('sample'),\n",
    ")\n",
    "\n",
    "sample_genome_gt50 = depth_df.filter(\n",
    "    pl.col('level').eq('total'),\n",
    "    pl.col('Breadth').gt(0.5)\n",
    ")['seqid_sample'].to_list()\n",
    "\n",
    "depth_df = depth_df.filter(pl.col('seqid_sample').is_in(sample_genome_gt50))\n",
    "\n",
    "# List of columns to ensure they exist\n",
    "required_columns = ['ASM001', 'ASM002', 'ASM003', 'BSM001', 'BSM002', \n",
    "                    'HSM001', 'HSM002', 'HSM003', 'HSM004', 'ZSM005', \n",
    "                    'ZSM025', 'ZSM027', 'ZSM028', 'ZSM031', 'ZSM101', \n",
    "                    'ZSM102', 'ZSM103', 'ZSM214', 'ZSM216', 'ZSM219']\n",
    "                    \n",
    "depth_df.sort(['sample', 'Breadth'], descending=[True, False]).filter(pl.col('seqid').eq('LR597635')).filter(\n",
    "    pl.col('sample').is_in(required_columns),\n",
    "    # pl.col('level').is_in(['species', 'total']),\n",
    "    pl.col('sample').eq('HSM001')\n",
    ")\n",
    "\n",
    "metadata = pl.read_csv(\n",
    "    '03-data/refdbs/ICTV/ICTV_database/20240209/ICTV_metadata.tsv',\n",
    "    separator=\"\\t\"\n",
    ")\n",
    "\n",
    "metadata = metadata.rename(\n",
    "    {\n",
    "        'Virus GENBANK accession': 'seqid',\n",
    "        'Host source': 'host',\n",
    "    }\n",
    ")\n",
    "\n",
    "seq2taxid = {}\n",
    "\n",
    "with open ('03-data/refdbs/centrifuge_db/ICTV_decoy/seqid2taxid.map') as f:\n",
    "    for line in f:\n",
    "        seqid, taxid = line.strip().split('\\t')\n",
    "        seq2taxid[seqid] = taxid\n",
    "\n",
    "metadata = metadata.with_columns(\n",
    "    taxid=pl.col('seqid').replace_strict(\n",
    "        seq2taxid,\n",
    "        default='0'\n",
    "    ),\n",
    ")\n",
    "\n",
    "metadata = metadata.rename(\n",
    "    {\n",
    "        'taxid': 'taxonomy_id',\n",
    "    }\n",
    ")\n",
    "\n",
    "sample_genome_species_gt5 = depth_df.filter(\n",
    "    pl.col('level').eq('species'),\n",
    "    pl.col('Breadth').gt(0.05)\n",
    ")['seqid_sample'].to_list()\n",
    "\n",
    "depth_df_total = depth_df.filter(\n",
    "    pl.col('level').eq('total'),\n",
    "    pl.col('seqid_sample').is_in(sample_genome_species_gt5)\n",
    ")\n",
    "\n",
    "depth_df_total = depth_df_total.join(metadata[['seqid', 'host', 'Species', 'Genus', 'taxonomy_id']].unique('seqid'), on='seqid', how='left')\n",
    "\n",
    "depth_df_total = depth_df_total.filter(~pl.col('taxonomy_id').is_null())\n",
    "\n",
    "# Using the polars dataframe with the depth and breadth information and the species, genus, taxid, sample and host information, I will for each pair genus/sample keep one the seqid with the higest breadth value\n",
    "# As I use bowtie with k=50, I could have reads that map more than one representative of the same genus, so I will keep the species with the highest breadth value\n",
    "depth_df_total = depth_df_total.sort(['Genus', 'sample', 'Breadth'], descending=True).unique(['Genus', 'sample']).sort(['Species'])\n",
    "\n",
    "depth_df_total = depth_df_total.pivot(\n",
    "    index=['taxonomy_id'],\n",
    "    on='sample',\n",
    "    values='Breadth',\n",
    "    aggregate_function='max',\n",
    ").fill_null(0)\n",
    "\n",
    "depth_df_total = depth_df_total.join(metadata[['seqid', 'host', 'Species', 'Genus', 'taxonomy_id']].unique('taxonomy_id'), on='taxonomy_id', how='left')\n",
    "\n",
    "depth_df_total = depth_df_total.drop_nulls(subset=['host'])\n",
    "\n",
    "# List of columns to ensure they exist\n",
    "required_columns = ['ASM001', 'ASM002', 'ASM003', 'BSM001', 'BSM002', \n",
    "                    'HSM001', 'HSM002', 'HSM003', 'HSM004', 'ZSM005', \n",
    "                    'ZSM025', 'ZSM027', 'ZSM028', 'ZSM031', 'ZSM101', \n",
    "                    'ZSM102', 'ZSM103', 'ZSM214', 'ZSM216', 'ZSM219']\n",
    "\n",
    "# Check and add missing columns\n",
    "for col in required_columns:\n",
    "    if col not in depth_df_total.columns:\n",
    "        depth_df_total = depth_df_total.with_columns(pl.lit(0).alias(col))\n",
    "\n",
    "\n",
    "# Now you can proceed with your operations\n",
    "depth_df_total = depth_df_total.select(\n",
    "    [\n",
    "        'seqid', 'Species', 'Genus', 'taxonomy_id', 'host'\n",
    "    ] + required_columns\n",
    ")\n",
    "\n",
    "depth_bool = depth_df_total.with_columns(\n",
    "                pl.col(\"^[ABHZ]SM[0-9][0-9][0-9]$\").gt(0.5) * 1\n",
    "            ).sort('Species')\n",
    "\n",
    "\n",
    "\n",
    "df_presence = depth_bool.rename(\n",
    "    {\n",
    "        'Species': 'name',\n",
    "    }\n",
    ")\n",
    "\n",
    "df_presence_prokaryote = df_presence.filter(pl.col('host').is_in(['bacteria', 'archaea'])).select(['name','^[ABHZ]SM[0-9][0-9][0-9]$' ]).unpivot(\n",
    "    on=cs.numeric(),\n",
    "    index='name',\n",
    "    value_name='presence',\n",
    "    variable_name='site',\n",
    ")\n",
    "\n",
    "df_presence_prokaryote = df_presence_prokaryote.with_columns(\n",
    "    pl.col('site').str.head(3)\n",
    ")\n",
    "\n",
    "df_presence_prokaryote_species = df_presence_prokaryote.group_by(['name']).agg(pl.col('presence').sum()).filter(pl.col('presence') > 0).sort('presence', descending=True)\n",
    "\n",
    "df_presence_prokaryote_species = df_presence_prokaryote_species.rename(\n",
    "    {\n",
    "        'presence': 'total',\n",
    "    }\n",
    ")\n",
    "\n",
    "df_presence_prokaryote = df_presence_prokaryote.group_by(['name', 'site']).agg(pl.col('presence').sum()).filter(pl.col('presence') > 0).sort('presence', descending=True)\n",
    "\n",
    "df_presence_prokaryote = df_presence_prokaryote_species.top_k(by='total', k=50).join(\n",
    "    df_presence_prokaryote, left_on='name', right_on='name', how='left'\n",
    ")\n",
    "\n",
    "df_presence_prokaryote = df_presence_prokaryote.with_columns(\n",
    "    pl.col('site').replace_strict(\n",
    "        {\n",
    "            'ASM': 'Arid West Cave',\n",
    "            'BSM': 'Boomerang Shelter',\n",
    "            'HSM': 'Hallstatt',\n",
    "            'ZSM': 'Zape',\n",
    "        }\n",
    "    )\n",
    ").rename(\n",
    "    {\n",
    "        'site': 'Site',\n",
    "        'presence': 'Number of samples the species is present in',\n",
    "        'name': 'Species',\n",
    "    }\n",
    ").sort(['total','Site'], descending=[True, False])\n",
    "\n",
    "df_presence_prokaryote = df_presence_prokaryote.filter(~pl.col('Species').eq('Sinsheimervirus phiX174'))\n",
    "\n",
    "\n",
    "mapped_species_first_threshold = depth_df.filter(\n",
    "    pl.col('level').eq('total'),\n",
    ").join(\n",
    "    metadata[['seqid', 'host', 'Species', 'Genus', 'taxonomy_id']].unique('seqid'), \n",
    "    on='seqid', \n",
    "    how='left'\n",
    ").filter(~pl.col('taxonomy_id').is_null())['Species'].unique().to_list()\n",
    "\n",
    "mapped_species_second_threshold = df_presence_prokaryote['Species'].unique().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0091822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from collections import Counter\n",
    "\n",
    "# Extract genus and species\n",
    "def get_genus_species(species_list):\n",
    "    return_list = []\n",
    "\n",
    "    for species in species_list:\n",
    "        if species == \"Sinsheimervirus phiX174\":\n",
    "            print(species)\n",
    "            continue\n",
    "\n",
    "        g, *s = species.split()\n",
    "        s = ' '.join(s)\n",
    "\n",
    "        if not g.endswith('virus'):\n",
    "            *g, s = species.split()\n",
    "            g = ' '.join(g)\n",
    "        \n",
    "        return_list.append((g, s))\n",
    "\n",
    "    return return_list\n",
    "\n",
    "taxonomy_species = df_presence_prokaryote['Species'].unique().to_list()\n",
    "\n",
    "taxonomy_genus_species = get_genus_species(taxonomy_species)\n",
    "mapped_first_threshold = get_genus_species(mapped_species_first_threshold)\n",
    "mapped_second_threshold = get_genus_species(mapped_species_second_threshold)\n",
    "\n",
    "lost_genus = set(g for g, s in taxonomy_genus_species) - set(g for g, s in mapped_first_threshold)\n",
    "\n",
    "# Group species by genus\n",
    "taxonomy_by_genus = defaultdict(list)\n",
    "first_threshold_by_genus = defaultdict(list)\n",
    "second_threshold_by_genus = defaultdict(list)\n",
    "\n",
    "for genus, species in taxonomy_genus_species:\n",
    "    if genus in lost_genus:\n",
    "        taxonomy_by_genus[\"Other\"].append(species)\n",
    "    else:\n",
    "        taxonomy_by_genus[genus].append(species)\n",
    "\n",
    "for genus, species in mapped_first_threshold:\n",
    "    first_threshold_by_genus[genus].append(species)\n",
    "\n",
    "for genus, species in mapped_second_threshold:\n",
    "    second_threshold_by_genus[genus].append(species)\n",
    "\n",
    "# Prepare node and link information\n",
    "genera = list(taxonomy_by_genus.keys())\n",
    "nodes = (\n",
    "    genera +\n",
    "    [\"Lost\"] +\n",
    "    genera +\n",
    "    [\"Mapped\", \"Lost\"]\n",
    ")\n",
    "\n",
    "genus_start_index = {genus: i for i, genus in enumerate(genera)}\n",
    "lost_middle_index = len(genera)\n",
    "genus_middle_index = {genus: i + len(genera) + 1 for i, genus in enumerate(genera)}\n",
    "mapped_index = len(genera) + len(genera) + 1\n",
    "lost_final_index = len(genera) + len(genera) + 2\n",
    "\n",
    "sources = []\n",
    "targets = []\n",
    "values = []\n",
    "\n",
    "# Add flows for each genus\n",
    "for genus, species_list in taxonomy_by_genus.items():\n",
    "    genus_start_idx = genus_start_index[genus]\n",
    "    genus_middle_idx = genus_middle_index[genus]\n",
    "\n",
    "    # First threshold transitions\n",
    "    mapped_first_count = len([s for s in species_list if s in first_threshold_by_genus.get(genus, [])])\n",
    "    lost_first_count = len([s for s in species_list if s not in first_threshold_by_genus.get(genus, [])])\n",
    "    \n",
    "    if mapped_first_count > 0:\n",
    "        sources.append(genus_start_idx)  # From genus start\n",
    "        targets.append(genus_middle_idx)  # To genus in middle\n",
    "        values.append(mapped_first_count)\n",
    "    \n",
    "    if lost_first_count > 0:\n",
    "        sources.append(genus_start_idx)  # From genus start\n",
    "        targets.append(lost_middle_index)  # To \"Lost (Middle)\"\n",
    "        values.append(lost_first_count)\n",
    "\n",
    "    # Second threshold transitions for species mapped at first threshold\n",
    "    mapped_second_count = len([s for s in species_list if s in second_threshold_by_genus.get(genus, [])])\n",
    "    not_mapped_second_count = len([s for s in species_list if s in first_threshold_by_genus.get(genus, []) and s not in second_threshold_by_genus.get(genus, [])])\n",
    "\n",
    "    if mapped_second_count > 0:\n",
    "        sources.append(genus_middle_idx)  # From genus in middle\n",
    "        targets.append(mapped_index)     # To \"Mapped (Final)\"\n",
    "        values.append(mapped_second_count)\n",
    "    \n",
    "    if not_mapped_second_count > 0:\n",
    "        sources.append(genus_middle_idx)  # From genus in middle\n",
    "        targets.append(lost_final_index)  # To \"Lost (Final)\"\n",
    "        values.append(not_mapped_second_count)\n",
    "\n",
    "# Add flows for species lost in the first threshold\n",
    "sources.append(lost_middle_index)  # From \"Lost (Middle)\"\n",
    "targets.append(lost_final_index)   # To \"Lost (Final)\"\n",
    "values.append(sum(len(s) for s in taxonomy_by_genus.values()) - len(mapped_species_first_threshold))\n",
    "\n",
    "# Create the Sankey diagram\n",
    "fig = go.Figure(go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=20,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=nodes,\n",
    "        color=[\"rgb(184,146,96)\"] * len(genera) + [\"rgb(177,0,67)\"] +\n",
    "              [\"rgb(99,149,0)\"] * len(genera) + [\"rgb(99,149,0)\", \"rgb(177,0,67)\"]\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=sources,\n",
    "        target=targets,\n",
    "        value=values,\n",
    "        color=[\n",
    "            \"rgba(99,149,0, 0.5)\" if t in genus_middle_index.values() else\n",
    "            \"rgba(177,0,67, 0.5)\" if t == lost_middle_index else\n",
    "            \"rgba(99,149,0, 0.5)\" if t == mapped_index else\n",
    "            \"rgba(177,0,67, 0.5)\" for t in targets\n",
    "        ]\n",
    "    )\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Sankey Diagram with Intermediate Genus Retention and Thresholds\",\n",
    "    font_size=10,\n",
    "    width=500,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "fig.write_image(\"05-results/taxonomic_annotation/empirical_data/plot/sankey_diagram.comparisonProfilevsMapping.nophiX.pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab6e6ad",
   "metadata": {},
   "source": [
    "### Figure 3B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49496777",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = (\n",
    "    so.Plot(df_presence_prokaryote, y=\"Species\", x=\"Number of samples the species is present in\", color=\"Site\")\n",
    "    .add(so.Bar(), so.Stack())\n",
    "    .theme(axes_style(\"whitegrid\") | plotting_context(\"talk\") | {\"grid.linestyle\": \":\", \"pdf.fonttype\": 42})\n",
    "    .scale(color=so.Nominal(values=[\"#930c02\", \"#5d8300\",\"#008f93\", \"#755092\"], #[\"#de1e10\", \"#7cae00\",\"#00bfc4\", \"#9c6ac2\"]\n",
    "                            order=[\"Arid West Cave\", \"Boomerang Shelter\", \"Hallstatt\", \"Zape\"]), #[\"ASM\", \"BSM\", \"HSM\", \"ZSM\"]\n",
    "            )\n",
    "    .layout(size=(10, 9)) # width, height\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c88542e",
   "metadata": {},
   "source": [
    "### Figure 3C - Depth of the reads on the genome MG711460 (Mushuvirus mushu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8309725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysam\n",
    "import os\n",
    "import polars as pl\n",
    "from Bio import SeqIO\n",
    "import seaborn as sns\n",
    "import seaborn.objects as so\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bam = \"05-results/taxonomic_annotation/empirical_data_bwa/sam2lca/ZSM103.viruses_decoy.sam2lca.total.sorted.bam\"\n",
    "reference = \"MG711460\"\n",
    "\n",
    "sequence = SeqIO.index(\"05-results/taxonomic_annotation/empirical_data/top_viruses/viruses.fasta\", \"fasta\")[reference].seq\n",
    "\n",
    "bamfile = pysam.AlignmentFile(bam, \"rb\")\n",
    "\n",
    "reference_size = bamfile.header.get_reference_length(reference)\n",
    "\n",
    "coverage = bamfile.count_coverage(reference, start=0, end=reference_size)\n",
    "\n",
    "coverage = [cov.tolist() for cov in coverage]\n",
    "\n",
    "coverage = pl.DataFrame(coverage, schema=['A', 'C', 'G', 'T']).with_columns(\n",
    "    total = pl.col('A') + pl.col('C') + pl.col('G') + pl.col('T'),\n",
    "    pos = pl.lit(range(0, reference_size)),\n",
    ")\n",
    "\n",
    "coverage = coverage.with_columns(\n",
    "    pl.concat_list(pl.col(['A', 'C', 'G', 'T']))\n",
    "    .list.arg_max()\n",
    "    .map_elements(lambda x: coverage.columns[x], return_dtype=pl.Utf8)\n",
    "    .alias('max_column'),\n",
    "    real_sequence = pl.Series(list(str(sequence))),\n",
    ")\n",
    "\n",
    "coverage = coverage.with_columns(\n",
    "    pl.when(\n",
    "        pl.col('total').eq(0),\n",
    "    ).then(pl.col('real_sequence')).otherwise(pl.col('max_column')).alias('max_column')\n",
    ")\n",
    "\n",
    "diff_base = coverage.filter(\n",
    "    ~pl.col('max_column').eq(pl.col('real_sequence')),\n",
    ")\n",
    "\n",
    "p = (\n",
    "    so.Plot(data=coverage, x=\"pos\", y=\"total\")\n",
    "    .add(so.Area(color=\".5\"))\n",
    "    # .add(so.Bar(edgewidth=1, alpha=1), data=diff_base, x=\"pos\", y=\"total\", color=\"max_column\")\n",
    "    .theme({\"axes.facecolor\": \"w\", \"axes.edgecolor\": \"slategray\"})\n",
    "    .layout(size=(15, 2))  # in inches * (dpi / 100)\n",
    ")\n",
    "\n",
    "p.save(\"05-results/taxonomic_annotation/empirical_data_bwa/depth_breadth/Mushu_coverage.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98df6649",
   "metadata": {},
   "source": [
    "## Figure 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f008af3b",
   "metadata": {},
   "source": [
    "### Figure 4A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a5ef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from pathlib import Path\n",
    "import seaborn.objects as so\n",
    "from seaborn import axes_style\n",
    "from seaborn import plotting_context\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "depth_df = []\n",
    "\n",
    "folder_depth = Path(\"05-results/taxonomic_annotation/empirical_data_bwa/depth_breadth/\")\n",
    "\n",
    "sorted_files = sorted(folder_depth.glob(\"*depth_breadth.tsv\"))\n",
    "\n",
    "for file in sorted_files:\n",
    "    depth_df.append(pl.read_csv(file, separator=\"\\t\"))\n",
    "    depth_df[-1] = depth_df[-1].with_columns(\n",
    "        pl.col('Breadth').cast(pl.Float32),\n",
    "        pl.col('Depth avg').cast(pl.Float32),\n",
    "        pl.col('Depth median').cast(pl.Float32),\n",
    "        sample=pl.lit(file.stem.split('.')[0]),\n",
    "        level=pl.lit(file.stem.split('.')[2]),\n",
    "    )\n",
    "\n",
    "depth_df = pl.concat(depth_df)\n",
    "\n",
    "depth_df = depth_df.rename(\n",
    "    {\n",
    "        \"Contig\": \"seqid\",\n",
    "        \"Breadth\": \"Breadth_reads\",\n",
    "        \"Depth avg\": \"Depth_avg_reads\",\n",
    "        \"Depth median\": \"Depth_median_reads\",\n",
    "    }\n",
    ")\n",
    "\n",
    "depth_df = depth_df.filter(~pl.col('level').eq('unclassified'))\n",
    "\n",
    "depth_df = depth_df.with_columns(\n",
    "    seqid_sample=pl.col('seqid') + pl.lit('_') + pl.col('sample'),\n",
    ")\n",
    "\n",
    "depth_df_reads = depth_df.filter(\n",
    "    pl.col('level').eq('total'),\n",
    "    ~pl.col('seqid').eq('J02482')\n",
    ")\n",
    "\n",
    "depth_df = []\n",
    "\n",
    "folder_depth = Path(\"05-results/taxonomic_annotation/empirical_data_bwa/vclust/depth_breadth/\")\n",
    "\n",
    "# sorted_files = sorted(folder_depth.glob(\"*vclust.ani.depth_breadth.tsv\"))\n",
    "sorted_files = sorted(folder_depth.glob(\"*_coverage.tsv\"))\n",
    "\n",
    "for file in sorted_files:\n",
    "    depth_df.append(pl.read_csv(file, separator=\"\\t\"))\n",
    "    depth_df[-1] = depth_df[-1].rename(\n",
    "        {\n",
    "            \"coverage\": \"Breadth\",\n",
    "            \"reference\": \"Contig\",\n",
    "        }\n",
    "    ).with_columns(\n",
    "        pl.col('Breadth').cast(pl.Float32) / 100,\n",
    "        # pl.col('Depth avg').cast(pl.Float32),\n",
    "        # pl.col('Depth median').cast(pl.Float32),\n",
    "        sample=pl.lit(file.stem.split('.')[0]),\n",
    "        # software=pl.lit(file.stem.split('.')[1]), # [1] for cmseq\n",
    "        software=pl.lit(file.stem.split('.')[2]), # [2] for vclust direct\n",
    "    )\n",
    "\n",
    "depth_df = pl.concat(depth_df)\n",
    "\n",
    "depth_df = depth_df.rename(\n",
    "    {\n",
    "        \"Contig\": \"seqid\",\n",
    "        \"Breadth\": \"Breadth_contigs\",\n",
    "    }\n",
    ")\n",
    "\n",
    "depth_df = depth_df.with_columns(\n",
    "    seqid_sample=pl.col('seqid') + pl.lit('_') + pl.col('sample'),\n",
    ")\n",
    "\n",
    "depth_df_contigs = depth_df.filter(\n",
    "    ~pl.col('seqid').eq('J02482')\n",
    ")\n",
    "\n",
    "depth_concat = depth_df_contigs.select(\n",
    "        ['seqid_sample', 'Breadth_contigs', 'software']\n",
    "    ).join(\n",
    "        depth_df_reads.select(\n",
    "            ['seqid_sample', 'Breadth_reads', 'Depth_avg_reads', 'Depth_median_reads']\n",
    "        ), \n",
    "        on='seqid_sample', how='left'\n",
    "    ).with_columns(\n",
    "        pl.col('Breadth_reads').fill_null(0),\n",
    "        pl.col('Breadth_contigs').fill_null(0),\n",
    "    )\n",
    "\n",
    "vclust_df = []\n",
    "\n",
    "folder_vclust = Path(\"05-results/taxonomic_annotation/empirical_data_bwa/vclust/\")\n",
    "\n",
    "sorted_files = sorted(folder_vclust.glob(\"*parquet\"))\n",
    "\n",
    "for file in sorted_files:\n",
    "    vclust_df.append(pl.read_parquet(file))\n",
    "    vclust_df[-1] = vclust_df[-1].with_columns(\n",
    "        sample=pl.lit(file.stem.split('.')[0]),\n",
    "        software=pl.lit(file.stem.split('.')[2]),\n",
    "    )\n",
    "\n",
    "vclust_df = pl.concat(vclust_df)\n",
    "\n",
    "vclust_df = vclust_df.with_columns(\n",
    "    seqid_sample=pl.col('reference') + pl.lit('_') + pl.col('sample'),\n",
    "    query_sample=pl.col('query') + pl.lit('_') + pl.col('sample'),\n",
    ")\n",
    "\n",
    "vclust_df = vclust_df.rename(\n",
    "    {\n",
    "        \"reference\": \"seqid\",\n",
    "    }\n",
    ")\n",
    "\n",
    "vclust_df_contigs = vclust_df.filter(\n",
    "    ~pl.col('seqid').eq('J02482'),\n",
    "    pl.col('query').str.starts_with('NODE')\n",
    ")\n",
    "\n",
    "pydamage_df = []\n",
    "\n",
    "folder_pydamage = Path(\"/mnt/archgen/microbiome_coprolite/palaeofaeces_ecoevol/04-analysis/pydamage/\")\n",
    "\n",
    "sorted_files = sorted(folder_pydamage.glob(\"[ABHZ]*.pydamage.tsv.gz\"))\n",
    "\n",
    "for file in sorted_files:\n",
    "    pydamage_df.append(pl.read_csv(file, separator=\"\\t\"))\n",
    "    pydamage_df[-1] = pydamage_df[-1].with_columns(\n",
    "        pl.col('CtoT-0').cast(pl.Float32),\n",
    "        pl.col('damage_model_pmax').cast(pl.Float32),\n",
    "        sample=pl.lit(file.stem.split('.')[0]),\n",
    "        software=pl.lit(file.stem.split('.')[1]),\n",
    "    )\n",
    "\n",
    "pydamage_df = pl.concat(pydamage_df)\n",
    "\n",
    "pydamage_df = pydamage_df.rename(\n",
    "    {\n",
    "       \"reference\" : \"query\",\n",
    "        \"CtoT-0\":\"Damage\",\n",
    "        # \"damage_model_pmax\":\"Damage\",\n",
    "    }\n",
    ")\n",
    "\n",
    "pydamage_df = pydamage_df.with_columns(\n",
    "    query_sample=pl.col('query') + pl.lit('_') + pl.col('sample'),\n",
    ").filter(\n",
    "    pl.col('Damage').gt(0)\n",
    ")\n",
    "\n",
    "df_damage = vclust_df.join(pydamage_df, on='query_sample', how='left')\n",
    "\n",
    "df_damage = df_damage.with_columns(\n",
    "    seqid_sample = pl.col('seqid') + pl.lit('_') + pl.col('sample')\n",
    ")\n",
    "\n",
    "df_damage = df_damage.filter(\n",
    "    ~pl.col('Damage').is_null(),\n",
    ")\n",
    "\n",
    "df_damage = df_damage.group_by(['seqid_sample']).agg(pl.col('Damage').mean())\n",
    "\n",
    "depth_concat = depth_concat.join(df_damage, on='seqid_sample', how='left')\n",
    "\n",
    "depth_concat = depth_concat.with_columns(\n",
    "    sample=pl.col('seqid_sample').str.split('_').list.get(1),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303be9d6",
   "metadata": {},
   "source": [
    "To calculate the length per reference:\n",
    "\n",
    "```bash\n",
    "for bam in *viruses_decoy.cleaned.processed.sorted.bam ; do echo \"--------------\"$bam\"-----------\" ; samtools view -F4 $bam | awk 'BEGIN { print \"Reference\\tAverage_Read_Length\" } $3!=\"*\" { tot[$3] += length($10); count[$3]++ } END { for(ref in tot) { printf \"%s\\t%.2f\\n\", ref, tot[ref]/count[ref] } }' > $(basename $bam .bam).avg.reads_length.tsv ; done\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518bc010",
   "metadata": {},
   "outputs": [],
   "source": [
    "reads_len_df = []\n",
    "\n",
    "folder_depth = Path(\"05-results/taxonomic_annotation/empirical_data_bwa/BAM_files\")\n",
    "\n",
    "sorted_files = sorted(folder_depth.glob(\"*.viruses_decoy.cleaned.processed.sorted.avg.reads_length.tsv\"))\n",
    "\n",
    "for file in sorted_files:\n",
    "    reads_len_df.append(pl.read_csv(file, separator=\"\\t\"))\n",
    "    reads_len_df[-1] = reads_len_df[-1].with_columns(\n",
    "        pl.col('Average_Read_Length').cast(pl.Float32),\n",
    "        sample=pl.lit(file.stem.split('.')[0]),\n",
    "    ).rename(\n",
    "        {\n",
    "            \"Reference\": \"seqid\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "reads_len_df = pl.concat(reads_len_df)\n",
    "\n",
    "reads_len_df = reads_len_df.with_columns(\n",
    "    seqid_sample=pl.col('seqid') + pl.lit('_') + pl.col('sample'),\n",
    ")\n",
    "\n",
    "reads_len_df_contigs = reads_len_df.filter(\n",
    "    ~pl.col('seqid').eq('J02482')\n",
    ")\n",
    "\n",
    "depth_concat = depth_concat.join(reads_len_df_contigs.select(['seqid_sample', 'Average_Read_Length']), on='seqid_sample', how='left')\n",
    "\n",
    "p = (\n",
    "    so.Plot(depth_concat.filter(~pl.col(\"Damage\").is_null()), x=\"Breadth_reads\", y=\"Breadth_contigs\", color=\"software\")\n",
    "    .add(so.Dot())\n",
    "    .add(so.Line(color=\"grey\"), data=pl.DataFrame({\"x\": [0, 1], \"y\": [0, 1]}), x=\"x\", y=\"y\")\n",
    "    # .scale(color=\"crest\")\n",
    "    .theme(axes_style(\"whitegrid\") | plotting_context(\"talk\") | {\"grid.linestyle\": \":\", \"pdf.fonttype\": 42})\n",
    "    .layout(size=(10, 10)) # width, height\n",
    "    .limit(x=(0, 1), y=(0, 1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea351da",
   "metadata": {},
   "source": [
    "### Figure 4B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1309c644",
   "metadata": {},
   "source": [
    "#### metSPAdes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d97b519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "import pytaxonkit\n",
    "from Bio import SeqIO\n",
    "from collections import defaultdict\n",
    "import polars.selectors as cs\n",
    "import seaborn.objects as so\n",
    "from seaborn import axes_style\n",
    "from seaborn import plotting_context\n",
    "\n",
    "depth_df = []\n",
    "\n",
    "folder_depth = Path(\"05-results/taxonomic_annotation/empirical_data_bwa/vclust/depth_breadth/\")\n",
    "\n",
    "# sorted_files = sorted(folder_depth.glob(\"*vclust.ani.depth_breadth.tsv\"))\n",
    "sorted_files = sorted(folder_depth.glob(\"*_coverage.tsv\"))\n",
    "\n",
    "for file in sorted_files:\n",
    "    depth_df.append(pl.read_csv(file, separator=\"\\t\"))\n",
    "    depth_df[-1] = depth_df[-1].rename(\n",
    "        {\n",
    "            \"coverage\": \"Breadth\",\n",
    "            \"reference\": \"Contig\",\n",
    "        }\n",
    "    ).with_columns(\n",
    "        pl.col('Breadth').cast(pl.Float32) / 100,\n",
    "        # pl.col('Depth avg').cast(pl.Float32),\n",
    "        # pl.col('Depth median').cast(pl.Float32),\n",
    "        sample=pl.lit(file.stem.split('.')[0]),\n",
    "        # software=pl.lit(file.stem.split('.')[1]), # [1] for cmseq\n",
    "        software=pl.lit(file.stem.split('.')[2]), # [2] for vclust direct\n",
    "    )\n",
    "\n",
    "depth_df = pl.concat(depth_df)\n",
    "\n",
    "depth_df = depth_df.rename(\n",
    "    {\n",
    "        \"Contig\": \"seqid\",\n",
    "    }\n",
    ")\n",
    "\n",
    "depth_df = depth_df.filter(\n",
    "    ~pl.col('seqid').eq('J02482')\n",
    ")\n",
    "\n",
    "depth_df = depth_df.with_columns(\n",
    "    seqid_sample=pl.col('seqid') + pl.lit('_') + pl.col('sample'),\n",
    ")\n",
    "\n",
    "depth_df = depth_df.filter(\n",
    "    pl.col('Breadth').gt(0.5),\n",
    "    pl.col('software').eq('metaspades'),\n",
    ")\n",
    "\n",
    "\n",
    "# List of columns to ensure they exist\n",
    "required_columns = ['ASM001', 'ASM002', 'ASM003', 'BSM001', 'BSM002', \n",
    "                    'HSM001', 'HSM002', 'HSM003', 'HSM004', 'ZSM005', \n",
    "                    'ZSM025', 'ZSM027', 'ZSM028', 'ZSM031', 'ZSM101', \n",
    "                    'ZSM102', 'ZSM103', 'ZSM214', 'ZSM216', 'ZSM219']\n",
    "                    \n",
    "\n",
    "\n",
    "metadata = pl.read_csv(\n",
    "    '03-data/refdbs/ICTV/ICTV_database/20240209/ICTV_metadata.tsv',\n",
    "    separator=\"\\t\"\n",
    ")\n",
    "\n",
    "metadata = metadata.rename(\n",
    "    {\n",
    "        'Virus GENBANK accession': 'seqid',\n",
    "        'Host source': 'host',\n",
    "    }\n",
    ")\n",
    "\n",
    "seq2taxid = {}\n",
    "\n",
    "with open ('03-data/refdbs/centrifuge_db/ICTV_decoy/seqid2taxid.map') as f:\n",
    "    for line in f:\n",
    "        seqid, taxid = line.strip().split('\\t')\n",
    "        seq2taxid[seqid] = taxid\n",
    "\n",
    "metadata = metadata.with_columns(\n",
    "    taxid=pl.col('seqid').replace_strict(\n",
    "        seq2taxid,\n",
    "        default='0'\n",
    "    ),\n",
    ")\n",
    "\n",
    "metadata = metadata.rename(\n",
    "    {\n",
    "        'taxid': 'taxonomy_id',\n",
    "    }\n",
    ")\n",
    "\n",
    "depth_df_total = depth_df.join(metadata[['seqid', 'host', 'Species', 'Genus', 'taxonomy_id']].unique('seqid'), on='seqid', how='left')\n",
    "\n",
    "depth_df_total = depth_df_total.filter(~pl.col('taxonomy_id').is_null())\n",
    "\n",
    "# To attribute the coverage to only one species, I will keep the seqid with the higest breadth value among the seqids that belong to the same genus\n",
    "depth_df_total = depth_df_total.sort(['Genus', 'sample', 'Breadth'], descending=True).unique(['Genus', 'sample']).sort(['Species'])\n",
    "\n",
    "depth_df_total = depth_df_total.pivot(\n",
    "    index=['taxonomy_id'],\n",
    "    on='sample',\n",
    "    values='Breadth',\n",
    "    aggregate_function='max',\n",
    ").fill_null(0)\n",
    "\n",
    "depth_df_total = depth_df_total.join(metadata[[\n",
    "    'seqid', 'host', 'Species', 'Genus', 'taxonomy_id'\n",
    "    ]].unique('taxonomy_id'), on='taxonomy_id', how='left')\n",
    "\n",
    "depth_df_total = depth_df_total.drop_nulls(subset=['host'])\n",
    "\n",
    "# List of columns to ensure they exist\n",
    "required_columns = ['ASM001', 'ASM002', 'ASM003', 'BSM001', 'BSM002', \n",
    "                    'HSM001', 'HSM002', 'HSM003', 'HSM004', 'ZSM005', \n",
    "                    'ZSM025', 'ZSM027', 'ZSM028', 'ZSM031', 'ZSM101', \n",
    "                    'ZSM102', 'ZSM103', 'ZSM214', 'ZSM216', 'ZSM219']\n",
    "\n",
    "# Check and add missing columns\n",
    "for col in required_columns:\n",
    "    if col not in depth_df_total.columns:\n",
    "        depth_df_total = depth_df_total.with_columns(pl.lit(0).alias(col))\n",
    "\n",
    "\n",
    "# Now you can proceed with your operations\n",
    "depth_df_total = depth_df_total.select(\n",
    "    [\n",
    "        'seqid', 'Species', 'Genus', 'taxonomy_id', 'host'\n",
    "    ] + required_columns\n",
    ")\n",
    "\n",
    "depth_bool = depth_df_total.with_columns(\n",
    "                pl.col(\"^[ABHZ]SM[0-9][0-9][0-9]$\").gt(0.5) * 1\n",
    "            ).sort('Species')\n",
    "\n",
    "\n",
    "df_presence = depth_bool.rename(\n",
    "    {\n",
    "        'Species': 'name',\n",
    "    }\n",
    ")\n",
    "\n",
    "df_presence_prokaryote = df_presence.filter(\n",
    "    pl.col('host').is_in(['bacteria', 'archaea'])\n",
    "    ).select(['name','^[ABHZ]SM[0-9][0-9][0-9]$' ]).unpivot(\n",
    "    on=cs.numeric(),\n",
    "    index='name',\n",
    "    value_name='presence',\n",
    "    variable_name='site',\n",
    ")\n",
    "\n",
    "df_presence_prokaryote = df_presence_prokaryote.with_columns(\n",
    "    pl.col('site').str.head(3)\n",
    ")\n",
    "\n",
    "df_presence_prokaryote_species = df_presence_prokaryote.group_by(['name']).agg(pl.col('presence').sum()).filter(pl.col('presence') > 0).sort('presence', descending=True)\n",
    "\n",
    "df_presence_prokaryote_species = df_presence_prokaryote_species.rename(\n",
    "    {\n",
    "        'presence': 'total',\n",
    "    }\n",
    ")\n",
    "\n",
    "df_presence_prokaryote = df_presence_prokaryote.group_by(['name', 'site']).agg(pl.col('presence').sum()).filter(pl.col('presence') > 0).sort('presence', descending=True)\n",
    "\n",
    "df_presence_prokaryote = df_presence_prokaryote_species.top_k(by='total', k=50).join(\n",
    "    df_presence_prokaryote, left_on='name', right_on='name', how='left'\n",
    ")\n",
    "\n",
    "df_presence_prokaryote = df_presence_prokaryote.with_columns(\n",
    "    pl.col('site').replace_strict(\n",
    "        {\n",
    "            'ASM': 'Arid West Cave',\n",
    "            'BSM': 'Boomerang Shelter',\n",
    "            'HSM': 'Hallstatt',\n",
    "            'ZSM': 'Zape',\n",
    "        }\n",
    "    )\n",
    ").rename(\n",
    "    {\n",
    "        'site': 'Site',\n",
    "        'presence': 'Number of samples the species is present in',\n",
    "        'name': 'Species',\n",
    "    }\n",
    ").sort(['total','Site'], descending=[True, False])\n",
    "\n",
    "df_presence_prokaryote = df_presence_prokaryote.filter(~pl.col('Species').eq('Sinsheimervirus phiX174'))\n",
    "\n",
    "p = (\n",
    "    # so.Plot(df_presence_prokaryote, y=\"name\", x=\"presence\", color=\"site\")\n",
    "    so.Plot(df_presence_prokaryote, y=\"Species\", x=\"Number of samples the species is present in\", color=\"Site\")\n",
    "    .add(so.Bar(), so.Stack())\n",
    "    .theme(axes_style(\"whitegrid\") | plotting_context(\"talk\") | {\"grid.linestyle\": \":\", \"pdf.fonttype\": 42})\n",
    "    .scale(color=so.Nominal(values=[\"#930c02\", \"#5d8300\",\"#008f93\", \"#755092\"], #[\"#de1e10\", \"#7cae00\",\"#00bfc4\", \"#9c6ac2\"]\n",
    "                            order=[\"Arid West Cave\", \"Boomerang Shelter\", \"Hallstatt\", \"Zape\"]), #[\"ASM\", \"BSM\", \"HSM\", \"ZSM\"]\n",
    "            )\n",
    "    .layout(size=(10, 9)) # width, height\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a8ee70",
   "metadata": {},
   "source": [
    "#### MEGAHIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cbaf20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "import pytaxonkit\n",
    "from Bio import SeqIO\n",
    "from collections import defaultdict\n",
    "import polars.selectors as cs\n",
    "import seaborn.objects as so\n",
    "from seaborn import axes_style\n",
    "from seaborn import plotting_context\n",
    "\n",
    "depth_df = []\n",
    "\n",
    "folder_depth = Path(\"05-results/taxonomic_annotation/empirical_data_bwa/vclust/depth_breadth/\")\n",
    "\n",
    "# sorted_files = sorted(folder_depth.glob(\"*vclust.ani.depth_breadth.tsv\"))\n",
    "sorted_files = sorted(folder_depth.glob(\"*_coverage.tsv\"))\n",
    "\n",
    "for file in sorted_files:\n",
    "    depth_df.append(pl.read_csv(file, separator=\"\\t\"))\n",
    "    depth_df[-1] = depth_df[-1].rename(\n",
    "        {\n",
    "            \"coverage\": \"Breadth\",\n",
    "            \"reference\": \"Contig\",\n",
    "        }\n",
    "    ).with_columns(\n",
    "        pl.col('Breadth').cast(pl.Float32) / 100,\n",
    "        # pl.col('Depth avg').cast(pl.Float32),\n",
    "        # pl.col('Depth median').cast(pl.Float32),\n",
    "        sample=pl.lit(file.stem.split('.')[0]),\n",
    "        # software=pl.lit(file.stem.split('.')[1]), # [1] for cmseq\n",
    "        software=pl.lit(file.stem.split('.')[2]), # [2] for vclust direct\n",
    "    )\n",
    "\n",
    "depth_df = pl.concat(depth_df)\n",
    "\n",
    "depth_df = depth_df.rename(\n",
    "    {\n",
    "        \"Contig\": \"seqid\",\n",
    "    }\n",
    ")\n",
    "\n",
    "depth_df_contigs = depth_df.filter(\n",
    "    ~pl.col('seqid').eq('J02482')\n",
    ")\n",
    "\n",
    "depth_df = depth_df.with_columns(\n",
    "    seqid_sample=pl.col('seqid') + pl.lit('_') + pl.col('sample'),\n",
    ")\n",
    "\n",
    "depth_df = depth_df.filter(\n",
    "    pl.col('Breadth').gt(0.5),\n",
    "    pl.col('software').eq('megahit'),\n",
    ")\n",
    "\n",
    "\n",
    "# List of columns to ensure they exist\n",
    "required_columns = ['ASM001', 'ASM002', 'ASM003', 'BSM001', 'BSM002', \n",
    "                    'HSM001', 'HSM002', 'HSM003', 'HSM004', 'ZSM005', \n",
    "                    'ZSM025', 'ZSM027', 'ZSM028', 'ZSM031', 'ZSM101', \n",
    "                    'ZSM102', 'ZSM103', 'ZSM214', 'ZSM216', 'ZSM219']\n",
    "                    \n",
    "\n",
    "\n",
    "metadata = pl.read_csv(\n",
    "    '03-data/refdbs/ICTV/ICTV_database/20240209/ICTV_metadata.tsv',\n",
    "    separator=\"\\t\"\n",
    ")\n",
    "\n",
    "metadata = metadata.rename(\n",
    "    {\n",
    "        'Virus GENBANK accession': 'seqid',\n",
    "        'Host source': 'host',\n",
    "    }\n",
    ")\n",
    "\n",
    "seq2taxid = {}\n",
    "\n",
    "with open ('03-data/refdbs/centrifuge_db/ICTV_decoy/seqid2taxid.map') as f:\n",
    "    for line in f:\n",
    "        seqid, taxid = line.strip().split('\\t')\n",
    "        seq2taxid[seqid] = taxid\n",
    "\n",
    "metadata = metadata.with_columns(\n",
    "    taxid=pl.col('seqid').replace_strict(\n",
    "        seq2taxid,\n",
    "        default='0'\n",
    "    ),\n",
    ")\n",
    "\n",
    "metadata = metadata.rename(\n",
    "    {\n",
    "        'taxid': 'taxonomy_id',\n",
    "    }\n",
    ")\n",
    "\n",
    "depth_df_total = depth_df.join(metadata[['seqid', 'host', 'Species', 'Genus', 'taxonomy_id']].unique('seqid'), on='seqid', how='left')\n",
    "\n",
    "depth_df_total = depth_df_total.filter(~pl.col('taxonomy_id').is_null())\n",
    "\n",
    "# To attribute the coverage to only one species, I will keep the seqid with the higest breadth value among the seqids that belong to the same genus\n",
    "depth_df_total = depth_df_total.sort(['Genus', 'sample', 'Breadth'], descending=True).unique(['Genus', 'sample']).sort(['Species'])\n",
    "\n",
    "depth_df_total = depth_df_total.pivot(\n",
    "    index=['taxonomy_id'],\n",
    "    on='sample',\n",
    "    values='Breadth',\n",
    "    aggregate_function='max',\n",
    ").fill_null(0)\n",
    "\n",
    "depth_df_total = depth_df_total.join(metadata[[\n",
    "    'seqid', 'host', 'Species', 'Genus', 'taxonomy_id'\n",
    "    ]].unique('taxonomy_id'), on='taxonomy_id', how='left')\n",
    "\n",
    "depth_df_total = depth_df_total.drop_nulls(subset=['host'])\n",
    "\n",
    "# List of columns to ensure they exist\n",
    "required_columns = ['ASM001', 'ASM002', 'ASM003', 'BSM001', 'BSM002', \n",
    "                    'HSM001', 'HSM002', 'HSM003', 'HSM004', 'ZSM005', \n",
    "                    'ZSM025', 'ZSM027', 'ZSM028', 'ZSM031', 'ZSM101', \n",
    "                    'ZSM102', 'ZSM103', 'ZSM214', 'ZSM216', 'ZSM219']\n",
    "\n",
    "# Check and add missing columns\n",
    "for col in required_columns:\n",
    "    if col not in depth_df_total.columns:\n",
    "        depth_df_total = depth_df_total.with_columns(pl.lit(0).alias(col))\n",
    "\n",
    "\n",
    "# Now you can proceed with your operations\n",
    "depth_df_total = depth_df_total.select(\n",
    "    [\n",
    "        'seqid', 'Species', 'Genus', 'taxonomy_id', 'host'\n",
    "    ] + required_columns\n",
    ")\n",
    "\n",
    "depth_bool = depth_df_total.with_columns(\n",
    "                pl.col(\"^[ABHZ]SM[0-9][0-9][0-9]$\").gt(0.5) * 1\n",
    "            ).sort('Species')\n",
    "\n",
    "\n",
    "df_presence = depth_bool.rename(\n",
    "    {\n",
    "        'Species': 'name',\n",
    "    }\n",
    ")\n",
    "\n",
    "df_presence_prokaryote = df_presence.filter(\n",
    "    pl.col('host').is_in(['bacteria', 'archaea'])\n",
    "    ).select(['name','^[ABHZ]SM[0-9][0-9][0-9]$' ]).unpivot(\n",
    "    on=cs.numeric(),\n",
    "    index='name',\n",
    "    value_name='presence',\n",
    "    variable_name='site',\n",
    ")\n",
    "\n",
    "df_presence_prokaryote = df_presence_prokaryote.with_columns(\n",
    "    pl.col('site').str.head(3)\n",
    ")\n",
    "\n",
    "df_presence_prokaryote_species = df_presence_prokaryote.group_by(['name']).agg(pl.col('presence').sum()).filter(pl.col('presence') > 0).sort('presence', descending=True)\n",
    "\n",
    "df_presence_prokaryote_species = df_presence_prokaryote_species.rename(\n",
    "    {\n",
    "        'presence': 'total',\n",
    "    }\n",
    ")\n",
    "\n",
    "df_presence_prokaryote = df_presence_prokaryote.group_by(['name', 'site']).agg(pl.col('presence').sum()).filter(pl.col('presence') > 0).sort('presence', descending=True)\n",
    "\n",
    "df_presence_prokaryote = df_presence_prokaryote_species.top_k(by='total', k=50).join(\n",
    "    df_presence_prokaryote, left_on='name', right_on='name', how='left'\n",
    ")\n",
    "\n",
    "df_presence_prokaryote = df_presence_prokaryote.with_columns(\n",
    "    pl.col('site').replace_strict(\n",
    "        {\n",
    "            'ASM': 'Arid West Cave',\n",
    "            'BSM': 'Boomerang Shelter',\n",
    "            'HSM': 'Hallstatt',\n",
    "            'ZSM': 'Zape',\n",
    "        }\n",
    "    )\n",
    ").rename(\n",
    "    {\n",
    "        'site': 'Site',\n",
    "        'presence': 'Number of samples the species is present in',\n",
    "        'name': 'Species',\n",
    "    }\n",
    ").sort(['total','Site'], descending=[True, False])\n",
    "\n",
    "df_presence_prokaryote = df_presence_prokaryote.filter(~pl.col('Species').eq('Sinsheimervirus phiX174'))\n",
    "\n",
    "p = (\n",
    "    # so.Plot(df_presence_prokaryote, y=\"name\", x=\"presence\", color=\"site\")\n",
    "    so.Plot(df_presence_prokaryote, y=\"Species\", x=\"Number of samples the species is present in\", color=\"Site\")\n",
    "    .add(so.Bar(), so.Stack())\n",
    "    .theme(axes_style(\"whitegrid\") | plotting_context(\"talk\") | {\"grid.linestyle\": \":\", \"pdf.fonttype\": 42})\n",
    "    .scale(color=so.Nominal(values=[\"#930c02\", \"#5d8300\",\"#008f93\", \"#755092\"], #[\"#de1e10\", \"#7cae00\",\"#00bfc4\", \"#9c6ac2\"]\n",
    "                            order=[\"Arid West Cave\", \"Boomerang Shelter\", \"Hallstatt\", \"Zape\"]), #[\"ASM\", \"BSM\", \"HSM\", \"ZSM\"]\n",
    "            )\n",
    "    .layout(size=(10, 9)) # width, height\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dac961",
   "metadata": {},
   "source": [
    "### Figure 4C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbf2889",
   "metadata": {},
   "source": [
    "Used the snakemake pipeline `PLOT_contigs_over_reference`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d12e37",
   "metadata": {},
   "source": [
    "## Figure 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956dd33c",
   "metadata": {},
   "source": [
    "### Figure 5A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5ae46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from pathlib import Path\n",
    "import gzip\n",
    "from Bio import SeqIO\n",
    "\n",
    "custom_style = {\"text.color\": \"#131516\",\n",
    "                \"svg.fonttype\": \"none\",\n",
    "                # \"font.family\": \"sans-serif\",\n",
    "                # \"font.weight\": \"light\",\n",
    "                \"axes.spines.right\": False,\n",
    "                \"axes.spines.top\": False,\n",
    "                # \"axes.spines.bottom\": False,\n",
    "                'xtick.bottom': False,\n",
    "                \"pdf.fonttype\": 42\n",
    "                }\n",
    "\n",
    "import seaborn as sns ; sns.set_theme(style=\"ticks\", rc=custom_style)  # for plot styling\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn.objects as so\n",
    "\n",
    "### Genome that were determine either contamination of containing some bacterial DNA for some not well defined provirus\n",
    "\n",
    "all_bacteria_ICTV = [\n",
    "    \"AE006468\",\n",
    "    \"AF049230\",\n",
    "    \"BD143114\",\n",
    "    \"BD269513\",\n",
    "    \"BX897699\",\n",
    "    \"CP000031\",\n",
    "    \"CP000830\",\n",
    "    \"CP001312\",\n",
    "    \"CP001357\",\n",
    "    \"CP006891\",\n",
    "    \"CP014526\",\n",
    "    \"CP015418\",\n",
    "    \"CP019275\",\n",
    "    \"CP023680\",\n",
    "    \"CP023686\",\n",
    "    \"CP038625\",\n",
    "    \"CP052639\",\n",
    "    \"JAEILC010000038\",\n",
    "    \"KK213166\",\n",
    "    \"M18706\",\n",
    "    \"QUVN01000024\",\n",
    "    \"U68072\",\n",
    "    \"U96748\",\n",
    "]\n",
    "provirus_not_well_defined = [\n",
    "    \"AY319521\",\n",
    "    \"EF462197\",\n",
    "    \"EF462198\",\n",
    "    \"EF710638\",\n",
    "    \"EF710642\",\n",
    "    \"FJ184280\",\n",
    "    \"FJ188381\",\n",
    "    \"HG424323\",\n",
    "    \"HM208303\",\n",
    "    \"J02013\",\n",
    "    \"JQ347801\",\n",
    "    \"K02712\",\n",
    "    \"KF147927\",\n",
    "    \"KF183314\",\n",
    "    \"KF183315\",\n",
    "    \"KP972568\",\n",
    "    \"KX232515\",\n",
    "    \"KX452695\",\n",
    "    \"MK075003\",\n",
    "    \"V01201\",\n",
    "]\n",
    "unverified = [\n",
    "    \"DQ188954\",\n",
    "    \"HM543472\",\n",
    "    \"JQ407224\",\n",
    "    \"KC008572\",\n",
    "    \"KC626021\",\n",
    "    \"KF302037\",\n",
    "    \"KF360047\",\n",
    "    \"KF938901\",\n",
    "    \"KJ641726\",\n",
    "    \"KM233624\",\n",
    "    \"KM389459\",\n",
    "    \"KM982402\",\n",
    "    \"KP843857\",\n",
    "    \"KR862307\",\n",
    "    \"KU343148\",\n",
    "    \"KU343149\",\n",
    "    \"KU343150\",\n",
    "    \"KU343151\",\n",
    "    \"KU343152\",\n",
    "    \"KU343153\",\n",
    "    \"KU343154\",\n",
    "    \"KU343155\",\n",
    "    \"KU343156\",\n",
    "    \"KU343160\",\n",
    "    \"KU343161\",\n",
    "    \"KU343162\",\n",
    "    \"KU343163\",\n",
    "    \"KU343164\",\n",
    "    \"KU343165\",\n",
    "    \"KU343169\",\n",
    "    \"KU343170\",\n",
    "    \"KU343171\",\n",
    "    \"KU672593\",\n",
    "    \"KU752557\",\n",
    "    \"KX098515\",\n",
    "    \"KX228196\",\n",
    "    \"KX228197\",\n",
    "    \"KX228198\",\n",
    "    \"KX363561\",\n",
    "    \"KX452696\",\n",
    "    \"KX452698\",\n",
    "    \"KX656670\",\n",
    "    \"KX656671\",\n",
    "    \"KX989546\",\n",
    "    \"KY450753\",\n",
    "    \"KY487839\",\n",
    "    \"KY608967\",\n",
    "    \"KY742649\",\n",
    "    \"MG459218\",\n",
    "    \"MG551742\",\n",
    "    \"MG599035\",\n",
    "    \"MH791395\",\n",
    "    \"MH791402\",\n",
    "    \"MH791405\",\n",
    "    \"MH791410\",\n",
    "    \"MH791412\",\n",
    "    \"MH918795\",\n",
    "    \"MH925094\",\n",
    "    \"MH992121\",\n",
    "    \"MK033136\",\n",
    "    \"MK050014\",\n",
    "    \"MK415316\",\n",
    "    \"MK415317\",\n",
    "    \"MK474470\",\n",
    "    \"MK780203\",\n",
    "    \"MN545971\",\n",
    "    \"MN871450\",\n",
    "    \"MN871491\",\n",
    "    \"MN871495\",\n",
    "    \"MN871498\",\n",
    "    \"MN928506\",\n",
    "    \"MT360681\",\n",
    "    \"MT360682\",\n",
    "    \"MW325771\",\n",
    "    \"MW685514\",\n",
    "    \"MW685515\",\n",
    "]\n",
    "\n",
    "all_unwanted = all_bacteria_ICTV + provirus_not_well_defined + unverified\n",
    "\n",
    "jeager_df = pl.read_csv(\n",
    "    \"05-results/taxonomic_annotation/empirical_data_bwa/simulation_genomad/jeager/*/*/*_default_phages_jaeger.tsv\",\n",
    "    separator=\"\\t\"\n",
    ")\n",
    "\n",
    "jeager_df = jeager_df.with_columns(\n",
    "    species_id = pl.col('contig_id').str.split('_').list.get(0)\n",
    ").filter(\n",
    "    ~pl.col('species_id').is_in(all_unwanted)\n",
    ")\n",
    "\n",
    "path_genomad = Path(\"05-results/taxonomic_annotation/empirical_data_bwa/simulation_genomad/genomad\")\n",
    "\n",
    "results = path_genomad.glob(\"*/*_summary/*_virus_summary.tsv\")\n",
    "\n",
    "genomad_df = []\n",
    "\n",
    "for file in results:\n",
    "    df = pl.read_csv(file, separator=\"\\t\")\n",
    "    df = df.with_columns(\n",
    "        species_id = pl.col('seq_name').str.split('_').list.get(0)\n",
    "    ).rename(\n",
    "        {\n",
    "            \"seq_name\": \"contig_id\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    genomad_df.append(df)\n",
    "\n",
    "genomad_df = pl.concat(genomad_df)\n",
    "\n",
    "genomad_df = genomad_df.filter(\n",
    "    ~pl.col('species_id').is_in(all_unwanted)\n",
    ")\n",
    "\n",
    "path_genomad_default = Path(\"05-results/taxonomic_annotation/empirical_data_bwa/simulation_genomad/genomad_default\")\n",
    "\n",
    "results = path_genomad_default.glob(\"*/*_summary/*_virus_summary.tsv\")\n",
    "\n",
    "genomad_default_df = []\n",
    "\n",
    "for file in results:\n",
    "    df = pl.read_csv(file, separator=\"\\t\")\n",
    "    df = df.with_columns(\n",
    "        pl.col('length').cast(pl.Int64),\n",
    "        species_id = pl.col('seq_name').str.split('_').list.get(0)\n",
    "    ).rename(\n",
    "        {\n",
    "            \"seq_name\": \"contig_id\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    genomad_default_df.append(df)\n",
    "\n",
    "genomad_default_df = pl.concat(genomad_default_df).filter(\n",
    "    ~pl.col('species_id').is_in(all_unwanted)\n",
    ")\n",
    "\n",
    "path2contigs = Path(\"03-data/simulated_viral_contigs\")\n",
    "\n",
    "list_contigs_num = []\n",
    "\n",
    "for file in path2contigs.glob(\"*.fna.gz\"):\n",
    "    with gzip.open(file, \"rb\") as f:\n",
    "        contigs = f.read()\n",
    "        count = contigs.count(b\">\")\n",
    "        list_contigs_num.append((file.name.split(\".\")[0], count))\n",
    "\n",
    "df_contigs_num = pl.DataFrame(list_contigs_num, schema=[\"species_id\", \"length\"], orient=\"row\")\n",
    "\n",
    "\n",
    "df_contigs_name = []\n",
    "\n",
    "for file in path2contigs.glob(\"*.fna.gz\"):\n",
    "    with gzip.open(file, \"rt\") as f:\n",
    "        parsed = SeqIO.parse(f, \"fasta\")\n",
    "        for record in parsed:\n",
    "            contig_name = record.id.replace(\"+\", \"F\").replace(\":-\", \"_R\").replace(\":\", \"_\").replace(\"-\", \"_\")\n",
    "            df_contigs_name.append((file.name.split(\".\")[0], contig_name, len(record.seq)))\n",
    "\n",
    "df_contigs_name = pl.DataFrame(df_contigs_name, schema=[\"species_id\", \"contig_id\", \"length\"], orient=\"row\")\n",
    "\n",
    "df_contigs_num = df_contigs_num.filter(\n",
    "    ~pl.col(\"species_id\").is_in(all_unwanted)\n",
    ")\n",
    "\n",
    "jeager_species_count = jeager_df.group_by(\"species_id\").agg(\n",
    "    pl.count(\"species_id\").alias(\"count_jeager\")\n",
    ")\n",
    "\n",
    "genomad_species_count = genomad_df.group_by(\"species_id\").agg(\n",
    "    pl.count(\"species_id\").alias(\"count_genomad\")\n",
    ")\n",
    "\n",
    "df_contigs_num = df_contigs_num.join(\n",
    "    jeager_species_count,\n",
    "    on=\"species_id\",\n",
    "    how=\"left\"\n",
    ").join(\n",
    "    genomad_species_count,\n",
    "    on=\"species_id\",\n",
    "    how=\"left\"\n",
    ").fill_null(0)\n",
    "\n",
    "df_contigs_num = df_contigs_num.with_columns(\n",
    "    pl.col(\"count_jeager\").cast(pl.Int64),\n",
    "    pl.col(\"count_genomad\").cast(pl.Int64)\n",
    ")\n",
    "\n",
    "df_contigs_name = df_contigs_name.with_columns(\n",
    "    genomad = pl.col(\"contig_id\").is_in(genomad_df[\"contig_id\"]),\n",
    "    jeager = pl.col(\"contig_id\").is_in(jeager_df[\"contig_id\"]),\n",
    "    genomad_default = pl.col(\"contig_id\").is_in(genomad_default_df[\"contig_id\"]),\n",
    ")\n",
    "\n",
    "# Assume df is your polars dataframe with columns:\n",
    "# \"species_id\", \"contig_id\", \"length\", \"genomad\", \"jeager\"\n",
    "\n",
    "# Melt the dataframe to long format so that we have a column \"method\" \n",
    "# (with values \"genomad\" or \"jeager\") and a column \"viral\" (True/False).\n",
    "df_long = df_contigs_name.unpivot(\n",
    "    index=[\"species_id\", \"contig_id\", \"length\"],\n",
    "    on=[\"genomad\", \"genomad_default\", \"jeager\"],\n",
    "    variable_name=\"method\",\n",
    "    value_name=\"viral\"\n",
    ")\n",
    "\n",
    "from seaborn import axes_style, plotting_context\n",
    "\n",
    "# Create the plot:\n",
    "# - x-axis: contig length\n",
    "# - color: detection method (genomad vs jeager)\n",
    "# - facet columns: viral status (True for detected, False for not detected)\n",
    "p = (\n",
    "    so.Plot(df_long, x=\"length\", color=\"method\")\n",
    "    .add(so.Bars(), so.Hist(bins=30))\n",
    "    .facet(col=\"viral\")\n",
    "    .theme(axes_style(\"whitegrid\") | plotting_context(\"talk\") | {\"grid.linestyle\": \":\", \"pdf.fonttype\": 42})\n",
    "    .layout(size=(10, 6))\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf4a022",
   "metadata": {},
   "source": [
    "### Figure 5B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3955f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "\n",
    "def plot_contig_matches(blast_tsv: str, outfig: str, list_viral: List[str], reference_id: str = \"MG711460\"):\n",
    "    \"\"\"\n",
    "    Plot contig matches against a reference sequence\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    blast_tsv : str\n",
    "        Path to TSV file containing blast results with columns query, reference, rstart, rend\n",
    "    outfig : str\n",
    "        Path to save the output figure\n",
    "    list_viral : List[str]\n",
    "        Name of the contigs detected as viral\n",
    "    reference_id : str\n",
    "        ID of the reference sequence to plot matches for (default: MG711460)\n",
    "    \"\"\"\n",
    "    \n",
    "    reference_len = 36636\n",
    "\n",
    "    # Read blast results\n",
    "    df = pl.read_csv(blast_tsv, separator=\"\\t\")\n",
    "    \n",
    "    # Filter for reference sequence\n",
    "    df = df.filter(pl.col(\"reference\").eq(reference_id))\n",
    "    \n",
    "    # Create plot\n",
    "    # Create the plot\n",
    "    p = plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Add reference line at y=0\n",
    "    plt.hlines(y=-0.9, xmin=0, xmax=reference_len, color='grey', linestyle='-', linewidth=5)\n",
    "\n",
    "    # Create y-position mapping for unique query names\n",
    "    unique_queries = sorted(df['query'].unique())\n",
    "    y_positions = {query: i for i, query in enumerate(unique_queries)}\n",
    "\n",
    "    # Plot rectangles for each match\n",
    "    for row in df.to_dicts():\n",
    "        y_pos = y_positions[row['query']]\n",
    "        if row['query'] in list_viral:\n",
    "            plt.hlines(y_pos, row['rstart'], row['rend'], linewidth=5, color='blue')\n",
    "        else:\n",
    "            plt.hlines(y_pos, row['rstart'], row['rend'], linewidth=5, color='red')\n",
    "        \n",
    "    # Customize plot\n",
    "    plt.grid(True, linestyle=':', alpha=0.3)\n",
    "    plt.ylabel('Contigs')\n",
    "    plt.xlabel('Position on reference')\n",
    "    plt.title(f'Contig matches on {reference_id}')\n",
    "    plt.yticks(range(len(unique_queries)), unique_queries)\n",
    "    plt.savefig(outfig, bbox_inches='tight')\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bf7601",
   "metadata": {},
   "outputs": [],
   "source": [
    "genomad_megahit_BSM001 = pl.read_csv(\n",
    "    \"05-results/taxonomic_annotation/empirical_data_bwa/genomad/BSM001-megahit/BSM001-megahit_summary/BSM001-megahit_virus_summary.tsv\",\n",
    "    separator=\"\\t\"\n",
    ")\n",
    "\n",
    "plot_contig_matches(\n",
    "    \"05-results/taxonomic_annotation/empirical_data_bwa/vclust/BSM001.contigs.megahit.aln.tsv\",\n",
    "    outfig = \"05-results/taxonomic_annotation/empirical_data_bwa/genomad/BSM001.contigs.megahit.genomad.pdf\",\n",
    "    list_viral = genomad_megahit_BSM001['seq_name'].unique().to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d58fdd",
   "metadata": {},
   "source": [
    "### Figure 5C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfc55a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from pathlib import Path\n",
    "import seaborn.objects as so\n",
    "from seaborn import axes_style, plotting_context\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "custom_style = {\"text.color\": \"#131516\",\n",
    "                \"svg.fonttype\": \"none\",\n",
    "                # \"font.family\": \"sans-serif\",\n",
    "                # \"font.weight\": \"light\",\n",
    "                \"axes.spines.right\": False,\n",
    "                \"axes.spines.top\": False,\n",
    "                # \"axes.spines.bottom\": False,\n",
    "                'xtick.bottom': False,\n",
    "                \"pdf.fonttype\": 42\n",
    "                }\n",
    "\n",
    "import seaborn as sns ; sns.set_theme(style=\"ticks\", rc=custom_style)  # for plot styling\n",
    "\n",
    "def plot_checkv_quality(files, genomad_folder, decoy):\n",
    "    # Initialize empty list to store dataframes\n",
    "    dfs = []\n",
    "    dfs_vclust = []\n",
    "    genomad_summaries = []\n",
    "    \n",
    "    # Process each file\n",
    "    for file in files:\n",
    "        # Get sample and software from parent folder name\n",
    "        sample, software = file.parent.name.split('-')\n",
    "        \n",
    "        # Read file and add sample/software columns\n",
    "        pydamage_csv = os.path.join(genomad_folder, f\"pydamage/{sample}-{software}/pydamage_filtered_results.csv\")\n",
    "        vclust_csv = os.path.join(genomad_folder, f\"{sample}-{software}/vclust/{sample}.contigs.{software}.aln.tsv\")\n",
    "        genomad_summary = os.path.join(genomad_folder, f\"{sample}-{software}/{sample}-{software}_summary/{sample}-{software}_virus_summary.tsv\")\n",
    "        \n",
    "        # remove df with no ancient DNA\n",
    "        if not os.path.exists(pydamage_csv):\n",
    "            continue\n",
    "\n",
    "        list_ancient = pl.read_csv(pydamage_csv)['reference'].to_list()\n",
    "\n",
    "        # Genomad df\n",
    "        df = pl.read_csv(file, separator=\"\\t\", null_values=[\"NA\", \"NaN\", \"nan\", \"N/A\", \"n/a\", \"N/A\"]).fill_null(0)\n",
    "        df = df.with_columns([\n",
    "            pl.lit(sample).alias('sample'),\n",
    "            pl.lit(software).alias('software')\n",
    "        ]).filter(\n",
    "            pl.col('contig_id').is_in(list_ancient),\n",
    "        ).with_columns(\n",
    "            pl.col('proviral_length').cast(pl.Int64),\n",
    "        )\n",
    "        \n",
    "        dfs.append(df)\n",
    "    \n",
    "        # Vclust df\n",
    "        df_vclust = pl.read_csv(vclust_csv, separator=\"\\t\")\n",
    "        df_vclust = df_vclust.with_columns([\n",
    "            pl.lit(sample).alias('sample'),\n",
    "            pl.lit(software).alias('software')\n",
    "        ]).filter(\n",
    "            pl.col('query').is_in(list_ancient),\n",
    "            pl.col('pident').gt(90),\n",
    "            pl.col(\"query\").str.starts_with('NODE'),\n",
    "            ~pl.col('reference').str.starts_with('NODE'),\n",
    "        ).unique(\n",
    "            subset=['query'], maintain_order=True\n",
    "        ).join(\n",
    "            df, left_on='query', right_on='contig_id', how='left'\n",
    "        )\n",
    "        \n",
    "        dfs_vclust.append(df_vclust)\n",
    "\n",
    "        # Genomad summary df\n",
    "        df_genomad = pl.read_csv(genomad_summary, separator=\"\\t\", null_values=[\"NA\", \"NaN\", \"nan\", \"N/A\", \"n/a\", \"N/A\"]).fill_null(0)\n",
    "        df_genomad = df_genomad.with_columns([\n",
    "            pl.lit(sample).alias('sample'),\n",
    "            pl.lit(software).alias('software')\n",
    "        ]).filter(\n",
    "            pl.col('seq_name').is_in(list_ancient),\n",
    "        ).with_columns(\n",
    "            pl.col('length').cast(pl.Int64),\n",
    "        ).rename(\n",
    "            {\n",
    "                \"seq_name\": \"contig_id\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        genomad_summaries.append(df_genomad)\n",
    "\n",
    "\n",
    "    # Combine all dataframes\n",
    "    combined_df = pl.concat(dfs)\n",
    "\n",
    "    combined_vclust_df = pl.concat(dfs_vclust)\n",
    "\n",
    "    combined_genomad_summary = pl.concat(genomad_summaries)\n",
    "    \n",
    "    contamination = combined_vclust_df.filter(\n",
    "        pl.col('reference').is_in(decoy),\n",
    "    )\n",
    "\n",
    "    nodes_contamination = contamination['query'].unique().to_list()\n",
    "\n",
    "    # Here we try to replace in checkv quality to bacterial contamination if contigs in nodes_contamination\n",
    "    combined_vclust_df = combined_vclust_df.with_columns(\n",
    "        checkv_quality_contamination=pl.col('query').replace_strict(\n",
    "            old=nodes_contamination,\n",
    "            new=\"Bacterial contamination\",\n",
    "            default=pl.col('checkv_quality')\n",
    "        )\n",
    "    ).filter(\n",
    "        ~pl.col(\"reference\").eq(\"J02482\"),\n",
    "    )\n",
    "\n",
    "    # print(combined_df.head())\n",
    "    combined_df = combined_df.with_columns(\n",
    "        checkv_quality_contamination=pl.col('contig_id').replace_strict(\n",
    "            old=nodes_contamination,\n",
    "            new=\"Bacterial contamination\",\n",
    "            default=pl.col('checkv_quality')\n",
    "        )\n",
    "    )\n",
    "\n",
    "    combined_df = combined_df.join(\n",
    "        combined_genomad_summary,\n",
    "        on=['contig_id', 'sample', 'software'], \n",
    "        how='left'\n",
    "    ).filter(\n",
    "        ~pl.col(\"taxonomy\").str.contains(\"Sinsheimervirus\"),\n",
    "    )\n",
    "\n",
    "    # Plot without the objects interface\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    p = sns.barplot(\n",
    "        combined_df.group_by(['software', 'checkv_quality_contamination']).len().sort(['software', 'checkv_quality_contamination']), \n",
    "        x='len', \n",
    "        y='checkv_quality_contamination', \n",
    "        hue='software', \n",
    "        dodge=True,\n",
    "        order=['Complete', 'High-quality', 'Medium-quality', 'Low-quality', 'Not-determined', 'Bacterial contamination'],\n",
    "        alpha=0.5,\n",
    "    )\n",
    "\n",
    "    p = sns.barplot(\n",
    "        combined_vclust_df.group_by(['software', 'checkv_quality_contamination']).len().sort(['software', 'checkv_quality_contamination']), \n",
    "        x='len', \n",
    "        y='checkv_quality_contamination', \n",
    "        hue='software', \n",
    "        dodge=True,\n",
    "        order=['Complete', 'High-quality', 'Medium-quality', 'Low-quality', 'Not-determined', 'Bacterial contamination'],\n",
    "        alpha=1,\n",
    "    )\n",
    "\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_xlabel(\"Number of contigs\")\n",
    "    ax.set_ylabel(\"CheckV quality\")\n",
    "    plt.xlim(0, 27000)\n",
    "    ax.bar_label(p.containers[0], fontsize=10, fmt='%.0f')\n",
    "    ax.bar_label(p.containers[1], fontsize=10, fmt='%.0f')\n",
    "    ax.bar_label(p.containers[2], fontsize=10, fmt='%.0f')\n",
    "    ax.bar_label(p.containers[3], fontsize=10, fmt='%.0f')\n",
    "\n",
    "    plt.savefig(os.path.join(\n",
    "        genomad_folder, \"checkv_quality.pdf\"\n",
    "    ), bbox_inches='tight')\n",
    "\n",
    "    return p, combined_vclust_df, combined_df, nodes_contamination\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c7631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "viral_decoy = SeqIO.index(\"05-results/taxonomic_annotation/empirical_data/top_viruses/viruses_decoy.fasta\", \"fasta\")\n",
    "viral = SeqIO.index(\"05-results/taxonomic_annotation/empirical_data/top_viruses/viruses.fasta\", \"fasta\")\n",
    "\n",
    "list_decoy = set(viral_decoy.keys()) - set(viral.keys())\n",
    "\n",
    "# some of the contigs in the decoy might be virus and or prophages\n",
    "putative_phage_in_decoy = pl.read_csv(\n",
    "    \"05-results/taxonomic_annotation/empirical_data_bwa/decoy/conservative/all_50_bacterial_decoy_summary/all_50_bacterial_decoy_virus_summary.tsv\",\n",
    "    separator=\"\\t\"\n",
    ")['seq_name'].unique().to_list()\n",
    "\n",
    "putative_phage_in_decoy = list(i.split('|')[0] for i in putative_phage_in_decoy)\n",
    "\n",
    "list_decoy = list_decoy - set(putative_phage_in_decoy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf635507",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_foler = Path(\"05-results/taxonomic_annotation/empirical_data_bwa/checkv_genomad/\")\n",
    "\n",
    "\n",
    "p, df_vclust, df_genomad, contam = plot_checkv_quality(\n",
    "    analysis_foler.glob(\"*/quality_summary.tsv\"),\n",
    "    genomad_folder = '05-results/taxonomic_annotation/empirical_data_bwa/genomad',\n",
    "    decoy = list_decoy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eef862",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_foler = Path(\"05-results/taxonomic_annotation/empirical_data_bwa/checkv_genomad_default/\")\n",
    "\n",
    "p, df_vclust_default, df_genomad_default, contam_default = plot_checkv_quality(\n",
    "    analysis_foler.glob(\"*/quality_summary.tsv\"),\n",
    "    genomad_folder = '05-results/taxonomic_annotation/empirical_data_bwa/genomad_default',\n",
    "    decoy = list_decoy\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9d07e9",
   "metadata": {},
   "source": [
    "### Figure SX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18f474c",
   "metadata": {},
   "source": [
    "It reused some dataframe from the figure 5C. To have the genomad with relaxed parameters just changed the name of the dataframe to df_genomad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740fcbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split taxonomy into different levels\n",
    "df_genomad_default = df_genomad_default.with_columns(\n",
    "    pl.col('taxonomy').replace_strict(\n",
    "        old=\"Unclassified\",\n",
    "        new=\";\"*8,\n",
    "        default=pl.col('taxonomy')\n",
    "    )\n",
    ").with_columns([\n",
    "    pl.col('taxonomy').str.split(';').list.get(0).alias('root'),\n",
    "    pl.col('taxonomy').str.split(';').list.get(1).alias('realm'),\n",
    "    pl.col('taxonomy').str.split(';').list.get(2).alias('kingdom'),\n",
    "    pl.col('taxonomy').str.split(';').list.get(3).alias('phylum'),\n",
    "    pl.col('taxonomy').str.split(';').list.get(4).alias('class'),\n",
    "    pl.col('taxonomy').str.split(';').list.get(5).alias('order'),\n",
    "    pl.col('taxonomy').str.split(';').list.get(6).alias('family'),\n",
    "    pl.col('taxonomy').str.split(';').list.get(7).alias('genus'),\n",
    "    pl.col('taxonomy').str.split(';').list.get(8).alias('species')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7d1937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First show a row with all taxonomy fields to verify the logic\n",
    "taxonomy_fields = ['root', 'realm', 'kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species']\n",
    "\n",
    "# Get the last non-empty taxonomic level for each row\n",
    "df_genomad_default = df_genomad_default.with_columns(\n",
    "    last_taxonomy=pl.concat_list(taxonomy_fields)\n",
    "    .list.eval(pl.element().filter(pl.element() != \"\"))\n",
    "    .list.last()\n",
    ").filter(\n",
    "    ~pl.col(\"contig_id\").is_in(df_vclust_default['query']),\n",
    ")\n",
    "\n",
    "# Create a frequency table first\n",
    "taxonomy_counts_default = df_genomad_default.filter(\n",
    "    pl.col(\"checkv_quality\").is_in(\n",
    "        [\"Complete\", \"High-quality\"]),\n",
    "    ).group_by([\"software\", \"last_taxonomy\"]).agg(pl.len().alias(\"count\")).sort(\"count\", descending=True)\n",
    "\n",
    "# Create the plot\n",
    "p = (\n",
    "    so.Plot(taxonomy_counts_default, y=\"last_taxonomy\", x=\"count\", color=\"software\")\n",
    "    .add(so.Bar(), so.Dodge())\n",
    "    .theme(axes_style(\"whitegrid\") | plotting_context(\"talk\") | {\"grid.linestyle\": \":\", \"pdf.fonttype\": 42})\n",
    "    .layout(size=(10, 8))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7f5c18",
   "metadata": {},
   "source": [
    "### Figure 5D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81052ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "def plot_contig_bins(blast_tsv: str, outfig: str, bin_df: pl.DataFrame, reference_id: str = \"MG711460\", reference_len: int = 36636):\n",
    "    \"\"\"\n",
    "    Plot contig bins against a reference sequence\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    blast_tsv : str\n",
    "        Path to TSV file containing blast results with columns query, reference, rstart, rend\n",
    "    outfig : str\n",
    "        Path to save the output figure\n",
    "    list_viral : List[str]\n",
    "        Name of the contigs detected as viral\n",
    "    reference_id : str\n",
    "        ID of the reference sequence to plot matches for (default: MG711460)\n",
    "    \"\"\"\n",
    "\n",
    "    # Read blast results\n",
    "    df = pl.read_csv(blast_tsv, separator=\"\\t\")\n",
    "    \n",
    "    # Filter for reference sequence\n",
    "    df = df.filter(pl.col(\"reference\").eq(reference_id))\n",
    "    \n",
    "    bin_df = bin_df.filter(\n",
    "        pl.col(\"bin\").is_in(\n",
    "            bin_df.group_by(\"bin\")\n",
    "            .agg(pl.len().alias(\"count\"))\n",
    "            .filter(pl.col(\"count\") > 1)\n",
    "            .select(\"bin\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Read binning results\n",
    "    bin_dict = bin_df.filter(\n",
    "        pl.col(\"contig\").is_in(df['query'].unique())\n",
    "    ).rows_by_key(key=[\"contig\"], unique=True)\n",
    "\n",
    "    # Get unique clusters in sorted order (for reproducibility)\n",
    "    unique_clusters = sorted(set(bin_dict.values()))\n",
    "    num_clusters = len(unique_clusters)\n",
    "\n",
    "    # Create a colormap (e.g., 'tab10', 'viridis', etc.)\n",
    "    if num_clusters <= 12:\n",
    "        cmap = plt.cm.get_cmap(\"Paired\", num_clusters)\n",
    "    else:\n",
    "        cmap = plt.cm.get_cmap(\"tab20\", num_clusters)\n",
    "\n",
    "    # Map each unique cluster to a hex color using the colormap\n",
    "    color_map = {cluster: mcolors.to_hex(cmap(i)) for i, cluster in enumerate(unique_clusters)}\n",
    "\n",
    "    # Map each contig name to its corresponding color\n",
    "    contig_colors = {contig: color_map[cluster] for contig, cluster in bin_dict.items()}\n",
    "\n",
    "    # Create plot\n",
    "    # Create the plot\n",
    "    p = plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Add reference line at y=0\n",
    "    plt.hlines(y=-0.9, xmin=0, xmax=reference_len, color='grey', linestyle='-', linewidth=5)\n",
    "\n",
    "    # Create y-position mapping for unique query names\n",
    "    unique_queries = sorted(df['query'].unique())\n",
    "    y_positions = {query: i for i, query in enumerate(unique_queries)}\n",
    "\n",
    "    # Plot rectangles for each match\n",
    "    for row in df.to_dicts():\n",
    "        y_pos = y_positions[row['query']]\n",
    "        color = contig_colors.get(row['query'], 'grey')\n",
    "        plt.hlines(y_pos, row['rstart'], row['rend'], linewidth=5, color=color)\n",
    "\n",
    "        name_cluster = f\"Cluster {bin_dict.get(row['query'], 'N/A')}\"\n",
    "        plt.text(row['rstart'] + (row['rend'] - row['rstart'])/2, y_pos + 0.5, name_cluster, fontsize=8, ha='center')\n",
    "        \n",
    "    # Customize plot\n",
    "    plt.grid(True, linestyle=':', alpha=0.3)\n",
    "    plt.ylabel('Contigs')\n",
    "    plt.xlabel('Position on reference')\n",
    "    plt.title(f'Contig matches on {reference_id}')\n",
    "    plt.yticks(range(len(unique_queries)), unique_queries)\n",
    "    plt.savefig(outfig, bbox_inches='tight')\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d68820",
   "metadata": {},
   "source": [
    "Here an example of how to plot for a specific case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead7669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = \"assembled\"\n",
    "software = \"metaspades\"\n",
    "sample = \"BSM001\"\n",
    "binning = \"semibin\"\n",
    "bin_file = \"contig_bins.tsv\"\n",
    "site = sample[:3]\n",
    "\n",
    "bin_df = pl.read_csv(\n",
    "    f\"05-results/taxonomic_annotation/empirical_data_bwa/binning/{method}/{software}/{binning}/{sample}-{software}/{bin_file}\", \n",
    "    separator=\"\\t\")\n",
    "\n",
    "plot_contig_bins(\n",
    "    f\"05-results/taxonomic_annotation/empirical_data_bwa/vclust/{sample}.contigs.{software}.aln.tsv\",\n",
    "    outfig = f\"05-results/taxonomic_annotation/empirical_data_bwa/genomad/{sample}.contigs.{software}.binning.{method}.{binning}.pdf\",\n",
    "    bin_df = bin_df\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
